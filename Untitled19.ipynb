{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36caaf-96db-4956-ad4d-09e642272922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee296e-69de-4a7d-8c66-7fae643b2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Great — I’ll build a full, ready-to-implement project plan + reproducible code snippets that cover the **entire pipeline** you described: data preprocessing, feature engineering (volatility & liquidity), model choices (regression & classification), training, evaluation (time-series aware), and deployment / interpretation. I won’t ask for more details — I’ll give a practical, sensible default workflow that works with a multi-symbol daily OHLCV + market cap dataset for 50+ cryptos.\n",
    "\n",
    "---\n",
    "\n",
    "# 1 — Problem framing (both useful)\n",
    "\n",
    "* **Regression**: predict **future numerical volatility** (e.g., next 7-day rolling std of log returns). Good when you need a magnitude.\n",
    "* **Classification**: predict volatility **regime** (e.g., Low / Medium / High) by splitting future volatility into quantiles (e.g., bottom 33%, mid 34%, top 33%). Good for signals, hedging, trading rules.\n",
    "\n",
    "I’ll provide code and guidance for both; you can pick one or run both and compare.\n",
    "\n",
    "---\n",
    "\n",
    "# 2 — Target & labels\n",
    "\n",
    "* Compute **daily log returns**: `r_t = ln(close_t / close_{t-1})`.\n",
    "* Compute rolling window volatility (std of log returns) with window `W_target` (e.g., 7 or 14 days). That is the **target** for regression.\n",
    "* For classification, convert the future `volatility_W_target` into classes by quantiles:\n",
    "\n",
    "  * Low: ≤ 33rd percentile\n",
    "  * Medium: 33–66\n",
    "  * High: > 66\n",
    "\n",
    "---\n",
    "\n",
    "# 3 — Data preprocessing (detailed)\n",
    "\n",
    "1. **Read, types & sort**\n",
    "\n",
    "   * `date` → `datetime`, ensure sorted by `symbol`, `date`.\n",
    "2. **Handle missing values**\n",
    "\n",
    "   * If entire day for a symbol is missing => drop that date/symbol row.\n",
    "   * Small gaps (1–3 days): forward fill `close`, `open`, `high`, `low` only after checking volume = 0? Prefer interpolation for prices: `df.groupby('symbol').apply(lambda x: x.interpolate(limit=3))`.\n",
    "   * For `volume` and `market_cap`: fill with 0 where appropriate OR forward-fill short gaps. If missing for long run, drop symbol or flag it.\n",
    "   * After feature creation, drop any rows with NaN in target or essential features.\n",
    "3. **Outliers / sanity checks**\n",
    "\n",
    "   * Remove rows with `close <= 0`, or unrealistic jumps (e.g., > 500% intraday) unless known forks/airdrops.\n",
    "4. **Resampling / alignment**\n",
    "\n",
    "   * Ensure all symbols aligned on trading calendar. Add missing dates with NaNs if you plan cross-sectional features.\n",
    "5. **Normalization & scaling**\n",
    "\n",
    "   * Use `StandardScaler` (zero mean/unit variance) or `QuantileTransformer` for non-Gaussian.\n",
    "   * For tree-based models scaling not necessary; for LSTM/NN do scaling per-feature (fit scaler on training only).\n",
    "6. **Leakage prevention**\n",
    "\n",
    "   * All rolling features must use only past data (use `.shift()` where needed). Train/test split must be time-based per symbol or overall.\n",
    "\n",
    "---\n",
    "\n",
    "# 4 — Feature engineering (volatility & liquidity focus)\n",
    "\n",
    "Per-symbol, compute:\n",
    "\n",
    "Price & return features\n",
    "\n",
    "* `log_return` = `ln(close / close.shift(1))`\n",
    "* `abs_return` = `|log_return|`\n",
    "* `range` = `(high - low) / close`\n",
    "* `close_open_pct` = `(close - open) / open`\n",
    "\n",
    "Rolling features (windows: 7, 14, 30)\n",
    "\n",
    "* `rolling_mean_return_W`\n",
    "* `rolling_std_return_W` (rolling realized volatility)\n",
    "* `rolling_max/min` of close\n",
    "* `rolling_skew/kurtosis` of returns\n",
    "\n",
    "Liquidity features\n",
    "\n",
    "* `volume` (raw)\n",
    "* `volume/market_cap` ratio\n",
    "* `rolling_avg_volume_W`\n",
    "* `turnover` = `volume / circulating_supply` if available (if not, use `volume/market_cap`)\n",
    "\n",
    "Technical indicators (examples)\n",
    "\n",
    "* ATR (Average True Range) — price volatility\n",
    "* Bollinger Bands width\n",
    "* RSI (14)\n",
    "* MACD and MACD hist\n",
    "\n",
    "Cross-sectional / market features (optional)\n",
    "\n",
    "* `market_return` = daily return of a crypto market index or BTC if index not available\n",
    "* `correlation_with_btc_W` (rolling correlation with BTC)\n",
    "* `rank_in_marketcap_day` (relative size)\n",
    "* `volatility_spread` = `symbol_vol - market_vol`\n",
    "\n",
    "Lagged features\n",
    "\n",
    "* lag volatility values (1, 3, 7 days)\n",
    "* lag returns (1,2,3 days)\n",
    "\n",
    "Feature note: compute these **per symbol** using `groupby('symbol')`.\n",
    "\n",
    "---\n",
    "\n",
    "# 5 — Train / validation split (time-series safe)\n",
    "\n",
    "* Use **walk-forward** (rolling) validation:\n",
    "\n",
    "  * Example: train on `t0:t1`, validate on `t1+1:t2`, test on `t2+1:t3`.\n",
    "* Or use **expanding window** CV per symbol or cross-sectionally.\n",
    "* Ensure scalers and any fit-statistics are computed only on the training window and then applied to validation/test.\n",
    "\n",
    "---\n",
    "\n",
    "# 6 — Models to try (practical shortlist)\n",
    "\n",
    "* **Baseline classical**:\n",
    "\n",
    "  * GARCH (arch package) on single-symbol volatility modeling (great baseline for volatility time-series).\n",
    "* **Tree-based (recommended for cross-sectional + engineered features)**:\n",
    "\n",
    "  * LightGBM / XGBoost / CatBoost (fast, robust, handles missing values).\n",
    "* **Neural networks**:\n",
    "\n",
    "  * LSTM / GRU on sequences (per-symbol sequences).\n",
    "  * 1D ConvNet (TCN) for sequence modelling.\n",
    "* **Hybrid**:\n",
    "\n",
    "  * Use GARCH output as a feature into LightGBM or NN.\n",
    "* **Ensemble** of the above.\n",
    "\n",
    "---\n",
    "\n",
    "# 7 — Evaluation metrics\n",
    "\n",
    "* **Regression**:\n",
    "\n",
    "  * MAE, RMSE, MAPE (careful with near-zero targets), R².\n",
    "  * Hit-rate of identifying high-volatility periods (e.g., top X%).\n",
    "* **Classification**:\n",
    "\n",
    "  * Accuracy, Precision, Recall, F1 for each class.\n",
    "  * ROC-AUC for binary (e.g., high vs non-high).\n",
    "* **Backtesting metrics (for strategy)**:\n",
    "\n",
    "  * P\\&L for a simple hedging/positioning rule triggered by predicted class.\n",
    "  * Maximum drawdown, Sharpe ratio of strategy.\n",
    "\n",
    "Also evaluate **calibration** (does predicted volatility magnitude match realized).\n",
    "\n",
    "---\n",
    "\n",
    "# 8 — Model explainability & risk\n",
    "\n",
    "* Use **SHAP** for tree models to identify top features driving volatility.\n",
    "* Importance of lagged volatility, volume/marketcap, ATR, BTC correlation usually high.\n",
    "* Monitor model drift: distributions shift quickly in crypto — set data-monitoring and re-training cadence (e.g., weekly/biweekly).\n",
    "\n",
    "---\n",
    "\n",
    "# 9 — Deployment ideas\n",
    "\n",
    "* Batch pipeline: daily run that ingests new data → update features → predict next `W_target` volatility → emit signals.\n",
    "* Provide outputs: per-symbol, predicted volatility number, probability of High class, recommended action (e.g., reduce exposure if High).\n",
    "* Model retrain schedule: rolling retrain every N days; but validate with walk-forward simulation.\n",
    "\n",
    "---\n",
    "\n",
    "# 10 — Reproducible code (end-to-end skeleton)\n",
    "\n",
    "Below is a practical Python pipeline you can run (pandas / scikit-learn / lightgbm / keras). Adapt file paths and parameters to your environment.\n",
    "\n",
    "```python\n",
    "# requirements:\n",
    "# pip install pandas numpy scikit-learn lightgbm ta arch shap tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import shap\n",
    "import math\n",
    "\n",
    "# --------- Load & basic cleaning ----------\n",
    "df = pd.read_csv(\"crypto_data.csv\")  # cols: date, symbol, open, high, low, close, volume, market_cap\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['symbol','date']).reset_index(drop=True)\n",
    "\n",
    "# Remove rows with nonpositive prices\n",
    "df = df[df['close'] > 0]\n",
    "\n",
    "# --------- Feature engineering per symbol ----------\n",
    "def add_features(g):\n",
    "    g = g.copy()\n",
    "    g['log_return'] = np.log(g['close'] / g['close'].shift(1))\n",
    "    g['abs_return'] = g['log_return'].abs()\n",
    "    g['range_pct'] = (g['high'] - g['low']) / g['close']\n",
    "    # rolling windows\n",
    "    for w in [7,14,30]:\n",
    "        g[f'vol_rtn_{w}'] = g['log_return'].rolling(window=w).std()\n",
    "        g[f'ret_mean_{w}'] = g['log_return'].rolling(window=w).mean()\n",
    "        g[f'vol_mean_volume_{w}'] = g['volume'].rolling(window=w).mean()\n",
    "        g[f'vol_corr_btc_{w}'] = np.nan  # placeholder if BTC returns available\n",
    "    # liquidity ratio\n",
    "    g['vol_mc_ratio'] = g['volume'] / (g['market_cap'] + 1e-9)\n",
    "    # lag features\n",
    "    g['lag_vol_1'] = g['vol_rtn_7'].shift(1)\n",
    "    g = g.fillna(method='ffill', limit=3)  # cautious ffill for small gaps\n",
    "    return g\n",
    "\n",
    "df = df.groupby('symbol', group_keys=False).apply(add_features).reset_index(drop=True)\n",
    "\n",
    "# --------- Target creation: future 7-day volatility (regression) ----------\n",
    "W_target = 7\n",
    "df['future_vol_7'] = df.groupby('symbol')['log_return'].rolling(window=W_target).std().shift(-W_target+1).reset_index(level=0, drop=True)\n",
    "# classification labels:\n",
    "quantiles = df.groupby('symbol')['future_vol_7'].transform(lambda x: pd.qcut(x, q=3, labels=False, duplicates='drop'))\n",
    "df['vol_class_3'] = quantiles\n",
    "\n",
    "# Drop rows with NaN targets\n",
    "df = df.dropna(subset=['future_vol_7'])\n",
    "\n",
    "# --------- Features & split ----------\n",
    "features = ['log_return', 'abs_return', 'range_pct', 'vol_rtn_7', 'ret_mean_7', 'vol_mc_ratio', 'lag_vol_1', 'volume']\n",
    "# if some features not present, filter:\n",
    "features = [f for f in features if f in df.columns]\n",
    "X = df[features]\n",
    "y_reg = df['future_vol_7']\n",
    "y_clf = df['vol_class_3'].astype('int')\n",
    "\n",
    "# Time-based train/test split: last 20% as test\n",
    "dates = df['date'].sort_values().unique()\n",
    "split_date = dates[int(len(dates)*0.8)]\n",
    "train_mask = df['date'] <= split_date\n",
    "X_train, X_test = X[train_mask], X[~train_mask]\n",
    "y_train, y_test = y_reg[train_mask], y_reg[~train_mask]\n",
    "\n",
    "# Scale (fit on train)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.fillna(0))\n",
    "X_test_scaled = scaler.transform(X_test.fillna(0))\n",
    "\n",
    "# --------- LightGBM regression ----------\n",
    "train_data = lgb.Dataset(X_train_scaled, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test_scaled, label=y_test, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_data, valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=1000, early_stopping_rounds=50, verbose_eval=50)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred = model.predict(X_test_scaled, num_iteration=model.best_iteration)\n",
    "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Test RMSE:\", rmse, \"MAE:\", mae)\n",
    "\n",
    "# --------- SHAP explainability ----------\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=features)\n",
    "\n",
    "# --------- Simple signal: mark high volatility predictions (top 20%)\n",
    "threshold = np.nanpercentile(y_pred, 80)\n",
    "signals = (y_pred >= threshold).astype(int)\n",
    "```\n",
    "\n",
    "> Note: This is a base example. For production, do rolling CV, hyperparameter tuning (Optuna), and per-symbol models / cross-sectional features.\n",
    "\n",
    "---\n",
    "\n",
    "# 11 — LSTM baseline (sequence model idea)\n",
    "\n",
    "* Prepare sequences per-symbol of length `seq_len` (e.g., 60 days), target is next-7-day volatility.\n",
    "* Scale per-feature, build LSTM with dropout, early stopping.\n",
    "* Use batches that mix symbols (or train per-symbol if enough data).\n",
    "\n",
    "---\n",
    "\n",
    "# 12 — Backtest strategy example (simple)\n",
    "\n",
    "* If predicted `vol_class` = High → reduce position / add hedge (e.g., lower exposure).\n",
    "* Simulate returns with rules and compute strategy Sharpe, drawdown.\n",
    "\n",
    "---\n",
    "\n",
    "# 13 — Operational considerations\n",
    "\n",
    "* Data quality: cryptos have forks, delistings → keep symbol metadata and drop tokens with sparse history.\n",
    "* Frequent model retraining: try weekly retrain + daily scoring.\n",
    "* Monitoring: track calibration drift and feature distributions.\n",
    "* Production infra: containerized inference (Docker), schedule with Airflow/Cron, store predictions in DB + dashboard (Grafana/Streamlit).\n",
    "\n",
    "---\n",
    "\n",
    "# 14 — Suggested deliverables I can prepare for you (pick any/all)\n",
    "\n",
    "* 1. A **Jupyter notebook** implementing the full pipeline above (cleaning → features → LightGBM → evaluation → SHAP).\n",
    "* 2. A **walk-forward evaluation report** with charts (predicted vs realized vol, confusion matrices, hit rates).\n",
    "* 3. A **simple strategy backtest** that uses the classification outputs and reports risk metrics.\n",
    "* 4. A **production-ready script** (Python) for daily scoring and retraining.\n",
    "* 5. An **explainability report** (SHAP) showing top drivers.\n",
    "\n",
    "---\n",
    "\n",
    "# 15 — Next step (I’ll choose a helpful default)\n",
    "\n",
    "I created a practical end-to-end plan and included runnable code you can adapt. If you want, I can now:\n",
    "\n",
    "* generate the full Jupyter notebook (ready to run) with additional visualizations and walk-forward CV, **or**\n",
    "* produce the LSTM sequence model notebook and dataset-prep code, **or**\n",
    "* implement the backtest + reporting notebook.\n",
    "\n",
    "Tell me which of the above deliverables you’d like **now** and I’ll produce the chosen notebook code (complete and runnable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86822bf-7a26-45e3-95bb-d28f8553797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crypto Volatility Prediction — Project Notebook & Deliverables\n",
    "\n",
    "> This document is a ready-to-use project blueprint, reproducible code snippets, deployment examples, and the required deliverables for predicting cryptocurrency volatility from OHLCV + market cap data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "**Goal:** Predict short-term cryptocurrency volatility (regression) and volatility regime (Low/Medium/High — classification) using historical daily OHLC, volume, and market cap for 50+ coins.\n",
    "\n",
    "**Primary target(s):**\n",
    "\n",
    "* `future_vol_W` = rolling realized volatility of log returns over window `W` (e.g., 7 days) — regression target.\n",
    "* `vol_class` = volatility regime (3 classes by quantiles) — classification target.\n",
    "\n",
    "**Main components delivered:**\n",
    "\n",
    "* Data preprocessing & cleaned dataset\n",
    "* Feature engineering focused on volatility & liquidity\n",
    "* Exploratory Data Analysis (EDA) report with visuals\n",
    "* Trained models + hyperparameter tuning pipeline\n",
    "* Model evaluation and backtest example\n",
    "* Local deployment (Streamlit app + Flask API) for testing\n",
    "* Project documentation: HLD, LLD, pipeline architecture and final report\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Repository / File structure (suggested)\n",
    "\n",
    "```\n",
    "crypto-vol-prediction/\n",
    "├── data/\n",
    "│   ├── raw/                 # original CSV(s)\n",
    "│   └── processed/           # cleaned / feature-engineered files (per symbol or merged)\n",
    "├── notebooks/\n",
    "│   ├── 01_data_prep.ipynb\n",
    "│   ├── 02_eda.ipynb\n",
    "│   ├── 03_modeling_lgbm.ipynb\n",
    "│   ├── 04_lstm.ipynb\n",
    "│   └── 05_backtest.ipynb\n",
    "├── src/\n",
    "│   ├── data_processing.py\n",
    "│   ├── features.py\n",
    "│   ├── models.py\n",
    "│   ├── train.py\n",
    "│   ├── tune_optuna.py\n",
    "│   └── predict.py\n",
    "├── deploy/\n",
    "│   ├── app_streamlit.py\n",
    "│   └── app_flask.py\n",
    "├── reports/\n",
    "│   ├── EDA_report.pdf\n",
    "│   ├── Final_Report.pdf\n",
    "│   ├── HLD.md\n",
    "│   └── LLD.md\n",
    "├── requirements.txt\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Processing & Feature Engineering (summary + code)\n",
    "\n",
    "### Key preprocessing steps\n",
    "\n",
    "1. Read CSV(s): `date, symbol, open, high, low, close, volume, market_cap`.\n",
    "2. Convert `date` to `datetime`, sort by `symbol, date`.\n",
    "3. Handle missing values:\n",
    "\n",
    "   * Small gaps (<=3 days): interpolation for prices, forward-fill for volume/market\\_cap if reasonable.\n",
    "   * Longer gaps or sparse symbols: drop or flag.\n",
    "4. Remove invalid rows: `close <= 0` or clearly erroneous values.\n",
    "5. Alignment: ensure consistent calendar if you intend cross-sectional features.\n",
    "\n",
    "### Important feature engineering (per symbol group)\n",
    "\n",
    "* `log_return = np.log(close / close.shift(1))`\n",
    "* Rolling volatility: `vol_rtn_W = log_return.rolling(W).std()` (W = 7, 14, 30)\n",
    "* Absolute return, ranges: `abs_return`, `range_pct = (high - low) / close`\n",
    "* Liquidity: `vol_mc_ratio = volume / (market_cap + 1e-9)`; `rolling_avg_volume_W`\n",
    "* Technicals: ATR, Bollinger Band width, RSI(14), MACD\n",
    "* Correlations: rolling correlation with BTC returns (if available)\n",
    "* Lag features: lagged volatility (`lag_vol_1`, `lag_vol_3`), lagged returns\n",
    "* GARCH feature: fit simple GARCH per coin and add predicted variance as feature (optional)\n",
    "\n",
    "### Example function (Python snippet)\n",
    "\n",
    "```python\n",
    "# src/features.py (snippet)\n",
    "import numpy as np\n",
    "\n",
    "def add_basic_features(df):\n",
    "    df = df.sort_values('date')\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['abs_return'] = df['log_return'].abs()\n",
    "    df['range_pct'] = (df['high'] - df['low']) / df['close']\n",
    "    df['vol_mc_ratio'] = df['volume'] / (df['market_cap'] + 1e-9)\n",
    "    for w in (7,14,30):\n",
    "        df[f'vol_rtn_{w}'] = df['log_return'].rolling(window=w).std()\n",
    "        df[f'ret_mean_{w}'] = df['log_return'].rolling(window=w).mean()\n",
    "        df[f'vol_avg_vol_{w}'] = df['volume'].rolling(window=w).mean()\n",
    "        df[f'lag_vol_{w}'] = df[f'vol_rtn_{w}'].shift(1)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Brief explanation of new features added:** (to include in deliverables)\n",
    "\n",
    "* `log_return`: normalized price movement; base for volatility calculation.\n",
    "* `vol_rtn_W`: realized volatility over window W — both used as features and target.\n",
    "* `vol_mc_ratio`: liquidity proxy; lower ratios often indicate low liquidity and higher realized volatility.\n",
    "* `range_pct` and `abs_return`: intraday volatility cues.\n",
    "* Rolling means & lags: capture short-term persistence and momentum in volatility.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Selection & Hyperparameter Tuning\n",
    "\n",
    "**Recommended primary model:** LightGBM (fast, handles heterogeneity and many features). Secondary models: XGBoost, CatBoost, LSTM (sequence-level).\n",
    "\n",
    "### Hyperparameter tuning (Optuna example)\n",
    "\n",
    "* Use `TimeSeriesSplit` or custom expanding-window split inside Optuna objective.\n",
    "* Tune `num_leaves`, `max_depth`, `learning_rate`, `min_data_in_leaf`, `feature_fraction`, `bagging_fraction`.\n",
    "\n",
    "```python\n",
    "# tune_optuna.py (core idea)\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 16),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 200),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    rmses = []\n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        dtrain = lgb.Dataset(X[train_idx], label=y[train_idx])\n",
    "        dval = lgb.Dataset(X[val_idx], label=y[val_idx])\n",
    "        bst = lgb.train(params, dtrain, valid_sets=[dval], early_stopping_rounds=50, verbose_eval=False)\n",
    "        preds = bst.predict(X[val_idx])\n",
    "        rmses.append(((y[val_idx] - preds)**2).mean()**0.5)\n",
    "    return sum(rmses) / len(rmses)\n",
    "```\n",
    "\n",
    "**Tuning tips:**\n",
    "\n",
    "* Run optimization with limited trials first (20–50) to get baseline, then expand.\n",
    "* Use per-symbol tuning for top coins if you plan specialized models.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Model Testing & Validation\n",
    "\n",
    "**Validation strategy:**\n",
    "\n",
    "* Walk-forward validation (rolling windows): train on `t0..t1`, validate on `t1+1..t2`, expand or roll forward.\n",
    "* Use per-symbol tests and aggregated cross-sectional tests.\n",
    "\n",
    "**Metrics:**\n",
    "\n",
    "* Regression: RMSE, MAE, R²; plus top-X% hit-rate for identifying high volatility.\n",
    "* Classification: accuracy, precision/recall/F1, confusion matrix, ROC-AUC (if binary).\n",
    "\n",
    "**Example evaluation snippet:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Model Deployment (Local testing)\n",
    "\n",
    "Two local deployment options are included: Streamlit (interactive UI for analysts) and Flask (lightweight API for programmatic access).\n",
    "\n",
    "### Streamlit app (simple)\n",
    "\n",
    "```python\n",
    "# deploy/app_streamlit.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "st.title('Crypto Volatility Predictor')\n",
    "model = joblib.load('../models/lgbm_model.pkl')\n",
    "scaler = joblib.load('../models/scaler.pkl')\n",
    "\n",
    "uploaded = st.file_uploader('Upload processed features CSV', type='csv')\n",
    "if uploaded:\n",
    "    df = pd.read_csv(uploaded)\n",
    "    X = df[model_features]\n",
    "    Xs = scaler.transform(X)\n",
    "    preds = model.predict(Xs)\n",
    "    df['pred_vol'] = preds\n",
    "    st.line_chart(df.set_index('date')['pred_vol'])\n",
    "    st.dataframe(df.head(50))\n",
    "```\n",
    "\n",
    "Run locally: `streamlit run deploy/app_streamlit.py` and open [http://localhost:8501](http://localhost:8501)\n",
    "\n",
    "### Flask API (simple)\n",
    "\n",
    "```python\n",
    "# deploy/app_flask.py\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = joblib.load('../models/lgbm_model.pkl')\n",
    "scaler = joblib.load('../models/scaler.pkl')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    json_data = request.get_json()\n",
    "    df = pd.DataFrame(json_data)\n",
    "    X = df[model_features]\n",
    "    Xs = scaler.transform(X)\n",
    "    preds = model.predict(Xs)\n",
    "    return jsonify({'predictions': preds.tolist()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "Run: `python deploy/app_flask.py` (localhost:5000)\n",
    "\n",
    "---\n",
    "\n",
    "## 7. EDA Report (what to include)\n",
    "\n",
    "**Summary statistics:** mean, std, min, max, skewness for price, returns, volume, market\\_cap.\n",
    "\n",
    "**Visualizations to produce:**\n",
    "\n",
    "* Time series plots of `close`, `log_return`, `vol_rtn_7` for selected coins.\n",
    "* Distribution (histogram / KDE) of returns and realized volatility.\n",
    "* Correlation heatmap between features (volume, vol, market cap, technical indicators).\n",
    "* Boxplots of `vol_rtn_7` by coin size quartile.\n",
    "* Rolling cross-correlation with BTC.\n",
    "\n",
    "Include rendered charts in `reports/EDA_report.pdf` and embed captions.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Project Documentation (HLD & LLD outlines)\n",
    "\n",
    "### HLD (High-Level Design) — include:\n",
    "\n",
    "* System overview: data ingestion → preprocessing → feature store → model training → inference → dashboard/API.\n",
    "* Components and responsibilities (ETL, feature engineering, model training, serving).\n",
    "* Data storage choices (CSV / Parquet / DB), model artifact storage (joblib / MLflow).\n",
    "* Scaling considerations (batch vs streaming), retrain cadence.\n",
    "\n",
    "### LLD (Low-Level Design) — include:\n",
    "\n",
    "* Data schemas for raw and processed tables.\n",
    "* Exact code flow for each module (pseudocode + function list).\n",
    "* Deployment details: container image, required env vars, ports, sample request/response for API.\n",
    "* Logging & monitoring hooks (prediction logs, feature drift alerts).\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Final Report (example structure)\n",
    "\n",
    "1. Executive Summary\n",
    "2. Data Description\n",
    "3. EDA Findings\n",
    "4. Feature Engineering\n",
    "5. Modeling Approach\n",
    "6. Model Performance\n",
    "7. Backtesting / Strategy Example\n",
    "8. Limitations & Risks\n",
    "9. Next Steps\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Model Optimization Checklist\n",
    "\n",
    "* [ ] Baseline model trained and evaluated\n",
    "* [ ] Optuna hyperparameter tuning finished (store best params)\n",
    "* [ ] Feature selection/pruning using SHAP or permutation importance\n",
    "* [ ] Ensemble or stacking tested\n",
    "* [ ] Calibration & threshold tuning for classification\n",
    "\n",
    "---\n",
    "\n",
    "## 11. How I will produce the deliverables (if you want me to auto-generate)\n",
    "\n",
    "I can produce (pick any or I'll default to all):\n",
    "\n",
    "* Jupyter notebooks (data\\_prep, EDA, modeling, backtest)\n",
    "* Trained LightGBM model artifact + scaler saved (joblib)\n",
    "* Streamlit and Flask app code (ready to run)\n",
    "* EDA report (PDF) and Final Report (PDF)\n",
    "* HLD and LLD markdown files\n",
    "\n",
    "If you want everything now, I will generate the **full Jupyter notebook** (single notebook that runs the full pipeline) and the **Streamlit app** code and package them in the canvas for download.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Notes & cautions\n",
    "\n",
    "* Crypto markets evolve quickly: schedule frequent retraining and monitor model drift.\n",
    "* Watch for data quirks: rebrands, delistings, forks, and volume anomalies.\n",
    "* Evaluate model performance on a per-coin basis as well as aggregated.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36fc0a-f255-44f0-a719-d1d81de763ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crypto Volatility Prediction — Project Notebook & Deliverables\n",
    "\n",
    "> This document is a ready-to-use project blueprint, reproducible code snippets, deployment examples, and the required deliverables for predicting cryptocurrency volatility from OHLCV + market cap data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "**Goal:** Predict short-term cryptocurrency volatility (regression) and volatility regime (Low/Medium/High — classification) using historical daily OHLC, volume, and market cap for 50+ coins.\n",
    "\n",
    "**Primary target(s):**\n",
    "\n",
    "* `future_vol_W` = rolling realized volatility of log returns over window `W` (e.g., 7 days) — regression target.\n",
    "* `vol_class` = volatility regime (3 classes by quantiles) — classification target.\n",
    "\n",
    "**Main components delivered:**\n",
    "\n",
    "* Data preprocessing & cleaned dataset\n",
    "* Feature engineering focused on volatility & liquidity\n",
    "* Exploratory Data Analysis (EDA) report with visuals\n",
    "* Trained models + hyperparameter tuning pipeline\n",
    "* Model evaluation and backtest example\n",
    "* Local deployment (Streamlit app + Flask API) for testing\n",
    "* Project documentation: HLD, LLD, pipeline architecture and final report\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Repository / File structure (suggested)\n",
    "\n",
    "```\n",
    "crypto-vol-prediction/\n",
    "├── data/\n",
    "│   ├── raw/                 # original CSV(s)\n",
    "│   └── processed/           # cleaned / feature-engineered files (per symbol or merged)\n",
    "├── notebooks/\n",
    "│   ├── 01_data_prep.ipynb\n",
    "│   ├── 02_eda.ipynb\n",
    "│   ├── 03_modeling_lgbm.ipynb\n",
    "│   ├── 04_lstm.ipynb\n",
    "│   └── 05_backtest.ipynb\n",
    "├── src/\n",
    "│   ├── data_processing.py\n",
    "│   ├── features.py\n",
    "│   ├── models.py\n",
    "│   ├── train.py\n",
    "│   ├── tune_optuna.py\n",
    "│   └── predict.py\n",
    "├── deploy/\n",
    "│   ├── app_streamlit.py\n",
    "│   └── app_flask.py\n",
    "├── reports/\n",
    "│   ├── EDA_report.pdf\n",
    "│   ├── Final_Report.pdf\n",
    "│   ├── HLD.md\n",
    "│   └── LLD.md\n",
    "├── requirements.txt\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Processing & Feature Engineering (summary + code)\n",
    "\n",
    "### Key preprocessing steps\n",
    "\n",
    "1. Read CSV(s): `date, symbol, open, high, low, close, volume, market_cap`.\n",
    "2. Convert `date` to `datetime`, sort by `symbol, date`.\n",
    "3. Handle missing values:\n",
    "\n",
    "   * Small gaps (<=3 days): interpolation for prices, forward-fill for volume/market\\_cap if reasonable.\n",
    "   * Longer gaps or sparse symbols: drop or flag.\n",
    "4. Remove invalid rows: `close <= 0` or clearly erroneous values.\n",
    "5. Alignment: ensure consistent calendar if you intend cross-sectional features.\n",
    "\n",
    "### Important feature engineering (per symbol group)\n",
    "\n",
    "* `log_return = np.log(close / close.shift(1))`\n",
    "* Rolling volatility: `vol_rtn_W = log_return.rolling(W).std()` (W = 7, 14, 30)\n",
    "* Absolute return, ranges: `abs_return`, `range_pct = (high - low) / close`\n",
    "* Liquidity: `vol_mc_ratio = volume / (market_cap + 1e-9)`; `rolling_avg_volume_W`\n",
    "* Technicals: ATR, Bollinger Band width, RSI(14), MACD\n",
    "* Correlations: rolling correlation with BTC returns (if available)\n",
    "* Lag features: lagged volatility (`lag_vol_1`, `lag_vol_3`), lagged returns\n",
    "* GARCH feature: fit simple GARCH per coin and add predicted variance as feature (optional)\n",
    "\n",
    "### Example function (Python snippet)\n",
    "\n",
    "```python\n",
    "# src/features.py (snippet)\n",
    "import numpy as np\n",
    "\n",
    "def add_basic_features(df):\n",
    "    df = df.sort_values('date')\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['abs_return'] = df['log_return'].abs()\n",
    "    df['range_pct'] = (df['high'] - df['low']) / df['close']\n",
    "    df['vol_mc_ratio'] = df['volume'] / (df['market_cap'] + 1e-9)\n",
    "    for w in (7,14,30):\n",
    "        df[f'vol_rtn_{w}'] = df['log_return'].rolling(window=w).std()\n",
    "        df[f'ret_mean_{w}'] = df['log_return'].rolling(window=w).mean()\n",
    "        df[f'vol_avg_vol_{w}'] = df['volume'].rolling(window=w).mean()\n",
    "        df[f'lag_vol_{w}'] = df[f'vol_rtn_{w}'].shift(1)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Brief explanation of new features added:** (to include in deliverables)\n",
    "\n",
    "* `log_return`: normalized price movement; base for volatility calculation.\n",
    "* `vol_rtn_W`: realized volatility over window W — both used as features and target.\n",
    "* `vol_mc_ratio`: liquidity proxy; lower ratios often indicate low liquidity and higher realized volatility.\n",
    "* `range_pct` and `abs_return`: intraday volatility cues.\n",
    "* Rolling means & lags: capture short-term persistence and momentum in volatility.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Selection & Hyperparameter Tuning\n",
    "\n",
    "**Recommended primary model:** LightGBM (fast, handles heterogeneity and many features). Secondary models: XGBoost, CatBoost, LSTM (sequence-level).\n",
    "\n",
    "### Hyperparameter tuning (Optuna example)\n",
    "\n",
    "* Use `TimeSeriesSplit` or custom expanding-window split inside Optuna objective.\n",
    "* Tune `num_leaves`, `max_depth`, `learning_rate`, `min_data_in_leaf`, `feature_fraction`, `bagging_fraction`.\n",
    "\n",
    "```python\n",
    "# tune_optuna.py (core idea)\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 16),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 200),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    rmses = []\n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        dtrain = lgb.Dataset(X[train_idx], label=y[train_idx])\n",
    "        dval = lgb.Dataset(X[val_idx], label=y[val_idx])\n",
    "        bst = lgb.train(params, dtrain, valid_sets=[dval], early_stopping_rounds=50, verbose_eval=False)\n",
    "        preds = bst.predict(X[val_idx])\n",
    "        rmses.append(((y[val_idx] - preds)**2).mean()**0.5)\n",
    "    return sum(rmses) / len(rmses)\n",
    "```\n",
    "\n",
    "**Tuning tips:**\n",
    "\n",
    "* Run optimization with limited trials first (20–50) to get baseline, then expand.\n",
    "* Use per-symbol tuning for top coins if you plan specialized models.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Model Testing & Validation\n",
    "\n",
    "**Validation strategy:**\n",
    "\n",
    "* Walk-forward validation (rolling windows): train on `t0..t1`, validate on `t1+1..t2`, expand or roll forward.\n",
    "* Use per-symbol tests and aggregated cross-sectional tests.\n",
    "\n",
    "**Metrics:**\n",
    "\n",
    "* Regression: RMSE, MAE, R²; plus top-X% hit-rate for identifying high volatility.\n",
    "* Classification: accuracy, precision/recall/F1, confusion matrix, ROC-AUC (if binary).\n",
    "\n",
    "**Example evaluation snippet:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Model Deployment (Local testing)\n",
    "\n",
    "Two local deployment options are included: Streamlit (interactive UI for analysts) and Flask (lightweight API for programmatic access).\n",
    "\n",
    "### Streamlit app (simple)\n",
    "\n",
    "```python\n",
    "# deploy/app_streamlit.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "st.title('Crypto Volatility Predictor')\n",
    "model = joblib.load('../models/lgbm_model.pkl')\n",
    "scaler = joblib.load('../models/scaler.pkl')\n",
    "\n",
    "uploaded = st.file_uploader('Upload processed features CSV', type='csv')\n",
    "if uploaded:\n",
    "    df = pd.read_csv(uploaded)\n",
    "    X = df[model_features]\n",
    "    Xs = scaler.transform(X)\n",
    "    preds = model.predict(Xs)\n",
    "    df['pred_vol'] = preds\n",
    "    st.line_chart(df.set_index('date')['pred_vol'])\n",
    "    st.dataframe(df.head(50))\n",
    "```\n",
    "\n",
    "Run locally: `streamlit run deploy/app_streamlit.py` and open [http://localhost:8501](http://localhost:8501)\n",
    "\n",
    "### Flask API (simple)\n",
    "\n",
    "```python\n",
    "# deploy/app_flask.py\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = joblib.load('../models/lgbm_model.pkl')\n",
    "scaler = joblib.load('../models/scaler.pkl')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    json_data = request.get_json()\n",
    "    df = pd.DataFrame(json_data)\n",
    "    X = df[model_features]\n",
    "    Xs = scaler.transform(X)\n",
    "    preds = model.predict(Xs)\n",
    "    return jsonify({'predictions': preds.tolist()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "Run: `python deploy/app_flask.py` (localhost:5000)\n",
    "\n",
    "---\n",
    "\n",
    "## 7. EDA Report (what to include)\n",
    "\n",
    "**Summary statistics:** mean, std, min, max, skewness for price, returns, volume, market\\_cap.\n",
    "\n",
    "**Visualizations to produce:**\n",
    "\n",
    "* Time series plots of `close`, `log_return`, `vol_rtn_7` for selected coins.\n",
    "* Distribution (histogram / KDE) of returns and realized volatility.\n",
    "* Correlation heatmap between features (volume, vol, market cap, technical indicators).\n",
    "* Boxplots of `vol_rtn_7` by coin size quartile.\n",
    "* Rolling cross-correlation with BTC.\n",
    "\n",
    "Include rendered charts in `reports/EDA_report.pdf` and embed captions.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Project Documentation (HLD & LLD outlines)\n",
    "\n",
    "### HLD (High-Level Design) — include:\n",
    "\n",
    "* System overview: data ingestion → preprocessing → feature store → model training → inference → dashboard/API.\n",
    "* Components and responsibilities (ETL, feature engineering, model training, serving).\n",
    "* Data storage choices (CSV / Parquet / DB), model artifact storage (joblib / MLflow).\n",
    "* Scaling considerations (batch vs streaming), retrain cadence.\n",
    "\n",
    "### LLD (Low-Level Design) — include:\n",
    "\n",
    "* Data schemas for raw and processed tables.\n",
    "* Exact code flow for each module (pseudocode + function list).\n",
    "* Deployment details: container image, required env vars, ports, sample request/response for API.\n",
    "* Logging & monitoring hooks (prediction logs, feature drift alerts).\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Final Report (example structure)\n",
    "\n",
    "1. Executive Summary\n",
    "2. Data Description\n",
    "3. EDA Findings\n",
    "4. Feature Engineering\n",
    "5. Modeling Approach\n",
    "6. Model Performance\n",
    "7. Backtesting / Strategy Example\n",
    "8. Limitations & Risks\n",
    "9. Next Steps\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Model Optimization Checklist\n",
    "\n",
    "* [ ] Baseline model trained and evaluated\n",
    "* [ ] Optuna hyperparameter tuning finished (store best params)\n",
    "* [ ] Feature selection/pruning using SHAP or permutation importance\n",
    "* [ ] Ensemble or stacking tested\n",
    "* [ ] Calibration & threshold tuning for classification\n",
    "\n",
    "---\n",
    "\n",
    "## 11. How I will produce the deliverables (if you want me to auto-generate)\n",
    "\n",
    "I can produce (pick any or I'll default to all):\n",
    "\n",
    "* Jupyter notebooks (data\\_prep, EDA, modeling, backtest)\n",
    "* Trained LightGBM model artifact + scaler saved (joblib)\n",
    "* Streamlit and Flask app code (ready to run)\n",
    "* EDA report (PDF) and Final Report (PDF)\n",
    "* HLD and LLD markdown files\n",
    "\n",
    "If you want everything now, I will generate the **full Jupyter notebook** (single notebook that runs the full pipeline) and the **Streamlit app** code and package them in the canvas for download.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Notes & cautions\n",
    "\n",
    "* Crypto markets evolve quickly: schedule frequent retraining and monitor model drift.\n",
    "* Watch for data quirks: rebrands, delistings, forks, and volume anomalies.\n",
    "* Evaluate model performance on a per-coin basis as well as aggregated.\n",
    "\n",
    "---\n",
    "\n",
    "*End of document.*\n",
    "\n",
    "---\n",
    "\n",
    "## Guidelines & Submission Requirements (Added)\n",
    "\n",
    "This section details exact submission, documentation, and deployment requirements to ensure the project meets evaluation standards.\n",
    "\n",
    "### Code Documentation\n",
    "\n",
    "* Every script in `src/` must include a module docstring describing purpose and usage.\n",
    "* Each function/class must include a short docstring describing inputs, outputs, and side effects.\n",
    "* Inline comments should explain non-obvious logic and assumptions.\n",
    "* Provide a `requirements.txt` and a `README.md` with setup & run instructions.\n",
    "\n",
    "### Report Structure & Content\n",
    "\n",
    "The final report must be clear, well-structured, and reproducible. Use the following sections:\n",
    "\n",
    "1. **Executive Summary** — 1 page summary of problem, approach, and key findings.\n",
    "2. **Data Description** — dataset sources, schema, time range, cleaning steps and missing-value handling.\n",
    "3. **EDA & Insights** — key visualizations and observations (trends, distributions, correlations).\n",
    "4. **Feature Engineering** — list of features added, rationale, and sample code snippets.\n",
    "5. **Modeling** — model choices, hyperparameters, tuning approach, and cross-validation strategy.\n",
    "6. **Evaluation** — metrics on test set, confusion matrix (for classification), error distribution (for regression), and per-coin performance summary.\n",
    "7. **Backtest / Use-case** — simple example of how predictions could inform risk decisions with performance metrics.\n",
    "8. **Deployment** — how to run the app locally, API spec, and retraining workflow.\n",
    "9. **Limitations & Future Work** — known caveats and suggested improvements.\n",
    "\n",
    "Include appendices for code excerpts, environment, and full hyperparameter settings.\n",
    "\n",
    "### Diagrams & Visuals\n",
    "\n",
    "Include the following diagrams and plots in the report:\n",
    "\n",
    "* Data flow diagram (ETL → feature store → model → serving).\n",
    "* Feature correlation heatmap and SHAP summary plot.\n",
    "* Time series plots of actual vs predicted volatility for sample coins.\n",
    "* Confusion matrix and precision/recall curves (for classification).\n",
    "* Model performance table across top 10 coins by market cap.\n",
    "\n",
    "### Deployment Requirements\n",
    "\n",
    "* Provide a local deployment option: **Streamlit** (interactive) and **Flask** (API). At minimum include:\n",
    "\n",
    "  * `deploy/app_streamlit.py` with instructions to run: `streamlit run deploy/app_streamlit.py`\n",
    "  * `deploy/app_flask.py` with an endpoint `/predict` accepting JSON payloads and returning predictions.\n",
    "* Save model artifact(s) under `models/` (e.g., `models/lgbm_model.pkl`, `models/scaler.pkl`). Use `joblib` or `pickle`.\n",
    "* Add a short `deploy/README.md` explaining how to test the API and the streamlit app (sample JSON and sample CSVs).\n",
    "\n",
    "### Submission Checklist (to include with final submission)\n",
    "\n",
    "* [ ] `notebooks/` (data\\_prep, EDA, modeling, backtest)\n",
    "* [ ] `src/` (well-documented scripts)\n",
    "* [ ] `deploy/` (Streamlit + Flask apps)\n",
    "* [ ] `models/` (saved model and scaler)\n",
    "* [ ] `reports/` (EDA\\_report.pdf, Final\\_Report.pdf, HLD.md, LLD.md)\n",
    "* [ ] `requirements.txt` and `README.md`\n",
    "* [ ] Simple test script `test_predict.py` demonstrating API usage\n",
    "\n",
    "---\n",
    "\n",
    "## Next steps I will perform now (automated)\n",
    "\n",
    "I will generate the following deliverables and add them to this canvas document as files/snippets in sequence:\n",
    "\n",
    "1. A **single runnable Jupyter notebook** (`notebooks/00_full_pipeline.ipynb`) that performs data loading, preprocessing, feature engineering, training (LightGBM), evaluation (walk-forward), SHAP explainability, and saves the trained model and scaler.\n",
    "2. A **Streamlit app** (`deploy/app_streamlit.py`) that loads the saved model and exposes a simple UI to upload processed feature CSVs and view predictions.\n",
    "3. A **Flask API** (`deploy/app_flask.py`) that exposes `/predict` for JSON inputs.\n",
    "4. `requirements.txt` listing required packages.\n",
    "\n",
    "I will include code comments and short usage instructions inside each file. After generating these, I will update this document's file structure section to include the newly generated files and provide quick run instructions.\n",
    "\n",
    "If you need any changes in the selection of deliverables, tell me now; otherwise I'll proceed with creating the notebook, Streamlit app, Flask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
